{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def623233067addc",
   "metadata": {},
   "source": [
    "This notebook is for visualizing each step of the CMMN process so that we can identify the intermediate representations of the whole thing before integrating into our setup.\n",
    "\n",
    "For now, I will eschew the class based formulation that the original authors provided and just implement everything iteratively, as Dr. B suggested.\n",
    "\n",
    "Also, this is now a branch of the ICWaves repo so that we can minimize any potential mistakes from separating things into a different package for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7590383838d3d5",
   "metadata": {},
   "source": [
    "Here are the main steps for translating cue -> emotion:\n",
    "\n",
    "1. Load in all source and target data. Emotion has 35 subj.s at 256 hz, cue has 12 subj.s at 500 hz. Load the resampled cue. Since this notebook is for visualization, for now do with a handful of subjects and then visualize with the full list.\n",
    "2. Calculate the normed barycenter of the source data (emotion). Viz.\n",
    "3. For each subject, calculate the filter to transform it to this barycenter. Viz.\n",
    "4. Convolve, visualize before and after of cue.\n",
    "5. Subj - subj filter calculating, mapping, and visualization as well."
   ]
  },
  {
   "cell_type": "code",
   "id": "fc946270470642c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:48:49.100385Z",
     "start_time": "2025-01-13T03:48:47.162639Z"
    }
   },
   "source": [
    "# imports\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, sosfilt, welch, freqz, sosfreqz\n",
    "from typing import List, Tuple"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "c4eb0e6b73bca160",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:48:49.125111Z",
     "start_time": "2025-01-13T03:48:49.115609Z"
    }
   },
   "source": [
    "# uncomment to viz whole thing\n",
    "# emotion_subj_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] # note subj. 22 is missing\n",
    "# cue_subj_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "# comment out to do more than a few. just a handful for testing the notebook\n",
    "emotion_subj_list = ['01', '02', '03', '04']\n",
    "frolich_subj_list = ['01', '02', '03', '04']"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a269c860aa3f0a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:48:49.144637Z",
     "start_time": "2025-01-13T03:48:49.134610Z"
    }
   },
   "source": [
    "# viz functions\n",
    "def psd(data, fs=256, nperseg=256):\n",
    "    \"\"\"\n",
    "    Compute the Power Spectral Density (PSD) of the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of numpy arrays, each containing EEG data for a subject\n",
    "    - fs: sampling frequency (default: 256 Hz)\n",
    "    - nperseg: length of each segment for Welch's method (default: 256)\n",
    "\n",
    "    Returns:\n",
    "    - f: array of sample frequencies (x-axis)\n",
    "    - Pxx: power spectral density of the data (y-axis)\n",
    "    \"\"\"\n",
    "\n",
    "    f, Pxx = welch(data, fs=fs, nperseg=nperseg)\n",
    "    return f, Pxx\n",
    "\n",
    "# Define a function to plot PSD\n",
    "def plot_psd(data, fs=256, nperseg=256, psds=None, title='PSD'):\n",
    "    \"\"\"\n",
    "    Plot the Power Spectral Density (PSD) of the given data.\n",
    "\n",
    "    Note: right now this is straight averaging, not the barycenter norming I do later.\n",
    "    If I want to do the norming, apply that first and then this will not average for me.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of numpy arrays, each containing EEG data for a subject\n",
    "    - fs: sampling frequency (default: 256 Hz)\n",
    "    - nperseg: length of each segment for Welch's method (default: 256)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, subj_data in enumerate(data):\n",
    "        # If subj_data is multi-dimensional, average across the channels\n",
    "\n",
    "        \"\"\"\n",
    "        Note, the below two are different because in the first I average the channels before taking the PSD,\n",
    "        and in the second I average the channels after taking the PSD.\n",
    "\n",
    "        Maybe should ask this to Dr. B and Carlos. Which is better since they do seem to result in some differences\n",
    "\n",
    "        In the rest, when calculating the barycenter and computing the filter, I average AFTER computing the PSD. Follow that convention.\n",
    "\n",
    "        Clean below later, but keep it as is for now to ask Dr. B and Carlos about it in the meeting later.\n",
    "        \"\"\"\n",
    "        if psds is None:\n",
    "            # If subj_data is multi-dimensional, average across the channels\n",
    "            if subj_data.ndim > 1:\n",
    "                subj_data = np.mean(subj_data, axis=0)\n",
    "            f, Pxx = psd(subj_data, fs=fs, nperseg=nperseg)\n",
    "\n",
    "            plt.plot(f, 10 * np.log10(Pxx), label=f'Subject {i+1}')\n",
    "        else:\n",
    "            if psds[i].ndim > 1:\n",
    "                psds[i] = np.mean(psds[i], axis=0)\n",
    "\n",
    "            f = np.linspace(0, 128, 129)\n",
    "\n",
    "            plt.plot(f, 10 * np.log10(psds[i]), label=f'Subject {i+1}')\n",
    "\n",
    "\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density (dB/Hz)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Structure the data as in the following template:\n",
    "\n",
    "- data\n",
    "- - emotion_256\n",
    "- - - raw_data_and_IC_labels\n",
    "- - - - subj-01.mat\n",
    "- - - - ...\n",
    "- - - - psds\n",
    "- - - - - subj-01_psds.npz\n",
    "- - frolich_256\n",
    "- - - frolich_extract_256_hz\n",
    "- - - - frolich_extract_01_256_hz.mat\n",
    "- - - - ...\n",
    "- - - - psds\n",
    "- - - - - frolich_extract_01_256_hz_psds.npz\n",
    "\n",
    "I have this data stored on Caviness under my own repositories / directories. Carlos you should also have access, ping me on Slack otherwise and I will guide you to it."
   ],
   "id": "81fed43a45f9407a"
  },
  {
   "cell_type": "code",
   "id": "16235a074227b9ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:22.094917Z",
     "start_time": "2025-01-13T03:48:49.332841Z"
    }
   },
   "source": [
    "# load in the data\n",
    "make_psds = False # change to True if running notebook for the first time. Note: this considerably lenghtens runtime\n",
    "emotion_filepath = Path('../data/emotion_256/raw_data_and_IC_labels')\n",
    "frolich_filepath = Path('../data/frolich_256/frolich_extract_256_hz')\n",
    "\n",
    "# quickly calculate and store the psds here before moving on\n",
    "\n",
    "# create psds directory\n",
    "if make_psds:\n",
    "    (emotion_filepath / 'psds').mkdir(parents=True, exist_ok=True)\n",
    "    (frolich_filepath / 'psds').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "emotion_data = []\n",
    "for subj in emotion_subj_list:\n",
    "    emotion_data.append(loadmat(emotion_filepath / f'subj-{subj}.mat')['data'])\n",
    "\n",
    "if make_psds:\n",
    "    for i, subj in enumerate(emotion_data):\n",
    "        f, Pxx = psd(subj)\n",
    "        np.savez(emotion_filepath / 'psds' / f'subj-{emotion_subj_list[i]}_psds', Pxx)\n",
    "\n",
    "emotion_data_psds_raw = []\n",
    "# # if a subdir to psds exists, load those so don't need to calculate here each time\n",
    "if (emotion_filepath / 'psds').exists():\n",
    "    for subj in emotion_subj_list:\n",
    "        emotion_data_psds_raw.append(np.load(emotion_filepath / 'psds' / f'subj-{subj}_psds.npz')['arr_0'])\n",
    "\n",
    "\n",
    "frolich_data = []\n",
    "for subj in frolich_subj_list:\n",
    "    frolich_data.append(loadmat(frolich_filepath / f'frolich_extract_{subj}_256_hz.mat')['X'])\n",
    "\n",
    "if make_psds:\n",
    "    for i, subj in enumerate(frolich_data):\n",
    "        f, Pxx = psd(subj)\n",
    "        np.savez(frolich_filepath / 'psds' / f'frolich_extract_{frolich_subj_list[i]}_256_hz_psds', Pxx)\n",
    "\n",
    "frolich_data_psds_raw = []\n",
    "# if a subdir to psds exists, load those so don't need to calculate here each time\n",
    "if (frolich_filepath / 'psds').exists():\n",
    "    for subj in frolich_subj_list:\n",
    "        frolich_data_psds_raw.append(np.load(frolich_filepath / 'psds' / f'frolich_extract_{subj}_256_hz_psds.npz')['arr_0'])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "d8a069d0640f7f7b",
   "metadata": {},
   "source": [
    "![title](notebook_figs/cmmn%20wasserstein%20barycenter%20form%20cropped.png)\n",
    "\n",
    "Here is the equation from the CMMN paper for barycenter matching. Note the hadamard square and square root. Also note that this is over subjects, not a method for averaging the channels.\n",
    "\n",
    "Equation 6 in the CMMN paper, page 14. For a full derivation, the equation is also in Appendix A, page 14."
   ]
  },
  {
   "cell_type": "code",
   "id": "580d07abf2c27dcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:22.988957Z",
     "start_time": "2025-01-13T03:49:22.971109Z"
    }
   },
   "source": [
    "# barycenter comp functions\n",
    "def compute_normed_barycenter(data, psds=None):\n",
    "    \"\"\"\n",
    "    Compute the normed barycenter of the given data.\n",
    "\n",
    "    I normalize by each subj's sum, as recommended by Dr. B on slack. -- Note, now commented out. Go with original CMMN instead\n",
    "\n",
    "    data: list of numpy arrays, each containing EEG data for a subject. Each array has shape (n_channels, n_samples).\n",
    "        Note: these should all be 256 hz, which emotion is natively and cue has been resampled to.\n",
    "    psds: list of numpy arrays, each containing the PSD of the data for a subject. Each array has shape (n_channels, n_freqs).\n",
    "    \"\"\"\n",
    "\n",
    "    # normalized_psds = []\n",
    "    # if psds is None:\n",
    "    #     psds = []\n",
    "    # for i, subj in enumerate(data):\n",
    "    #     if psds is None:\n",
    "    #         f, Pxx = psd(subj)\n",
    "    #         psds.append(Pxx)\n",
    "    #         normalized_psds.append(Pxx / np.sum(Pxx))\n",
    "    #     else:\n",
    "    #         normalized_psds.append(psds[i] / np.sum(psds[i]))\n",
    "    #\n",
    "    # # now average all together\n",
    "    # per_subj_avgs = []\n",
    "    # for subj in normalized_psds:\n",
    "    #     avg = np.mean(subj, axis=0)\n",
    "    #     per_subj_avgs.append(np.mean(subj, axis=0)) # necessary due to inhomogenous dimensions\n",
    "    #\n",
    "    # barycenter = np.mean(per_subj_avgs, axis=0)\n",
    "\n",
    "    # average per subj psds\n",
    "    per_subj_avgs = []\n",
    "    for subj in psds:\n",
    "        avg = np.mean(subj, axis=0)\n",
    "        per_subj_avgs.append(avg)\n",
    "\n",
    "    hadamard_square_root = []\n",
    "    for subj in per_subj_avgs:\n",
    "        hadamard_square_root.append(np.sqrt(subj))\n",
    "\n",
    "    averaged = np.zeros(hadamard_square_root[0].shape)\n",
    "    for subj in hadamard_square_root:\n",
    "        averaged += subj\n",
    "\n",
    "    averaged /= len(hadamard_square_root)\n",
    "\n",
    "    barycenter = np.square(averaged) # elementwise square\n",
    "\n",
    "    return barycenter\n",
    "\n",
    "def plot_barycenter(barycenter):\n",
    "    \"\"\"\n",
    "    Plot the normed barycenter of the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - barycenter: numpy array containing the normed barycenter of the data\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    f = np.linspace(0, 128, 129)\n",
    "    plt.plot(f, 10 * np.log10(barycenter))\n",
    "\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Power Spectral Density (dB/Hz)')\n",
    "    plt.title('Normed Barycenter')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e066cab979c14b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:23.127554Z",
     "start_time": "2025-01-13T03:49:23.100935Z"
    }
   },
   "source": [
    "# Computing the mappings\n",
    "def compute_filter(data, barycenter, psds=None):\n",
    "    \"\"\"\n",
    "    Compute the filter to transform the given data to the barycenter.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of numpy arrays, each containing EEG data for a subject in the target data (cue / frolich here)\n",
    "    - barycenter: numpy array containing the normed barycenter of the data\n",
    "\n",
    "    Returns:\n",
    "    - freq_filter: numpy array containing the filter in the frequency domain\n",
    "    - time_filter: numpy array containing the filter in the time domain\n",
    "    \"\"\"\n",
    "\n",
    "    # The filter in the frequency domain is barycenter / sqrt of each target subj's PSD\n",
    "    # Remember, this is not yet subj. to subj. matching. Each subj. is transformed with the same filter. See CMMN paper for details.\n",
    "\n",
    "    # first get target subj per-subj average to not deal with each channel individually\n",
    "    # calculate psds for each subj\n",
    "    if psds is None:\n",
    "        psds = []\n",
    "        for subj in data:\n",
    "            f, Pxx = psd(subj)\n",
    "            psds.append(Pxx)\n",
    "\n",
    "\n",
    "    avg_psds_per_subj = [\n",
    "        np.mean(subj, axis=0) for subj in psds\n",
    "    ]\n",
    "\n",
    "    freq_filter_per_subj_explicit = []\n",
    "    for avg_psd_per_subj in avg_psds_per_subj:\n",
    "        freq_filter = np.sqrt(barycenter) / np.sqrt(avg_psd_per_subj)\n",
    "        freq_filter_per_subj_explicit.append(freq_filter)\n",
    "\n",
    "    time_filter_per_subj_explicit = []\n",
    "    for subj in freq_filter_per_subj_explicit:\n",
    "        time_filter = np.fft.ifft(subj)\n",
    "        time_filter_per_subj_explicit.append(time_filter)\n",
    "\n",
    "    return freq_filter_per_subj_explicit, time_filter_per_subj_explicit\n",
    "\n",
    "def plot_freq_filter(freq_filter, fs=256):\n",
    "    \"\"\"\n",
    "    Plot the filter in the frequency domain.\n",
    "\n",
    "    Parameters:\n",
    "    - freq_filter: numpy array containing the filter in the frequency domain\n",
    "    - fs: sampling frequency (default: 256 Hz)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    f = np.linspace(0, fs/2, freq_filter[0].shape[-1])\n",
    "    for i, subj in enumerate(freq_filter):\n",
    "        plt.plot(f, 10 * np.log10(np.abs(subj.T)), label=f'Subject {i+1}')\n",
    "\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Filter (dB)')\n",
    "    plt.title('Frequency Filters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_time_filter(time_filter, fs=256):\n",
    "    \"\"\"\n",
    "    Plot the filter in the time domain.\n",
    "\n",
    "    Parameters:\n",
    "    - time_filter: numpy array containing the filter in the time domain\n",
    "    - fs: sampling frequency (default: 256 Hz)\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    t = np.arange(len(time_filter[0])) / fs\n",
    "    for i, subj in enumerate(time_filter):\n",
    "        plt.plot(t, np.real(subj), label=f'Subject {i+1}')\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Time Domain Filter')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c8e50489ed849da1",
   "metadata": {},
   "source": [
    "Adding the subj -> subj matching below. This will not use the barycenter calculation from the original paper. Here we instead just take the average PSD of each subject, and then find the closest matching subj in the source domain. Then, compute individual filters for that subject and match appropriately.\n",
    "\n",
    "Later, this will be wrapped up into a class obj. with more careful tracking.\n",
    "\n",
    "What is the distance metric of choice? Dr. B recommended me to do the square root of the Euclidean distance between PSDs, but I don't remember the rationale behind that. Should ask him."
   ]
  },
  {
   "cell_type": "code",
   "id": "48f3555206255b3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:23.191300Z",
     "start_time": "2025-01-13T03:49:23.177308Z"
    }
   },
   "source": [
    "def subj_subj_matching(source_psds, target_psds) -> List[int]:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_psds: list of numpy arrays, each containing the PSD of the data for a subject in the source domain. shape (n_channels, n_freqs)\n",
    "    target_psds: list of numpy arrays, each containing the PSD of the data for a subject in the target domain. shape (n_channels, n_freqs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of indices, where the i-th element is the index of the source subject that best matches the i-th target subject.\n",
    "    Ex: target subj 1 and 2 match to source domain 1, target subj 3 matches to source domain 2: [1, 1, 2]\n",
    "    \"\"\"\n",
    "\n",
    "    # Note the below does not average over channels first, but should still be identical to doing so\n",
    "\n",
    "    subj_subj_matches = []\n",
    "    for target_psd in target_psds:\n",
    "        min_dist = float('inf')\n",
    "        min_idx = -1\n",
    "        for i, source_psd in enumerate(source_psds):\n",
    "            dist = np.sqrt(np.sum((np.sqrt(source_psd) - np.sqrt(target_psd))**2))\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_idx = i\n",
    "        subj_subj_matches.append(min_idx)\n",
    "\n",
    "    return subj_subj_matches\n",
    "\n",
    "def compute_filter_subj_subj(data, subj_subj_matches, psds=None):\n",
    "    \"\"\"\n",
    "    Compute the filter to transform the given data to the barycenter, with subject to subject matching.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of numpy arrays, each containing EEG data for a subject in the target data (cue / frolich here)\n",
    "    - subj_subj_matches: list of indices, where the i-th element is the index of the source subject that best matches the i-th target subject\n",
    "    - psds: list of numpy arrays, each containing the PSD of the data for a subject. Each array has shape (n_channels, n_freqs).\n",
    "        Note that the data and psds are for the TARGET domain, here the frolich / cue data.\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate psds for each subj\n",
    "    if psds is None:\n",
    "        psds = []\n",
    "        for subj in data:\n",
    "            f, Pxx = psd(subj)\n",
    "            psds.append(Pxx)\n",
    "\n",
    "    freq_filter_per_subj_explicit = []\n",
    "    time_filter_per_subj_explicit = []\n",
    "\n",
    "    for i, subj in enumerate(data):\n",
    "        subj_psd = psds[i]\n",
    "        source_psd = psds[subj_subj_matches[i]]\n",
    "\n",
    "        freq_filter = np.sqrt(source_psd) / np.sqrt(subj_psd)\n",
    "        time_filter = np.fft.ifft(freq_filter)\n",
    "\n",
    "        freq_filter_per_subj_explicit.append(freq_filter)\n",
    "        time_filter_per_subj_explicit.append(time_filter)\n",
    "\n",
    "    return freq_filter_per_subj_explicit, time_filter_per_subj_explicit"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "faf23db79157684b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:23.284259Z",
     "start_time": "2025-01-13T03:49:23.268472Z"
    }
   },
   "source": [
    "# Computing the transformed data\n",
    "def transform_data(data, time_filter):\n",
    "    \"\"\"\n",
    "    Transform the given data using the given filter.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of numpy arrays, each containing EEG data for a subject in the target data\n",
    "    - time_filter: list containing the filter in the time domain for each subject\n",
    "\n",
    "    Returns:\n",
    "    - transformed_data: list of numpy arrays, each containing the transformed EEG data for a subject\n",
    "    \"\"\"\n",
    "\n",
    "    # Do the transformation on a channel basis\n",
    "\n",
    "    transformed_data = []\n",
    "    for i, subj in enumerate(data):\n",
    "        subj_norm = np.zeros(subj.shape)\n",
    "        num_channels = subj.shape[0]\n",
    "\n",
    "        for chan in range(num_channels):\n",
    "            subj_norm[chan] = np.convolve(subj[chan], time_filter[i], mode='same')\n",
    "\n",
    "        transformed_data.append(subj_norm)\n",
    "\n",
    "    return transformed_data"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "47fd650a91681d7a",
   "metadata": {},
   "source": [
    "## Computation and Plotting\n",
    "\n",
    "Now that all data has been loaded and all functions are instantiated, the below code cells calculate and visualize each step."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f9da3f62666fc1f",
   "metadata": {},
   "source": [
    "# plot the PSD of the emotion data (averaged over channels)\n",
    "plot_psd(emotion_data, psds=emotion_data_psds_raw, title='Emotion Data')"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "50d56687a97ba034",
   "metadata": {},
   "source": [
    "# plot the normed barycenter of the emotion data\n",
    "normed_emotion_barycenter = compute_normed_barycenter(emotion_data, psds=emotion_data_psds_raw)\n",
    "plot_barycenter(normed_emotion_barycenter)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "f0513c980c07ee14",
   "metadata": {},
   "source": [
    "# plot the PSD of the frolich data (averaged over channels)\n",
    "plot_psd(frolich_data, psds=frolich_data_psds_raw, title='Frolich Data')"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "d1cf74ec409a7b2c",
   "metadata": {},
   "source": [
    "Dr. B says zero out 60 hz spike in emotion by doing a notch filter first. Then wouldn't see this big spike in the emotion that the frequency filter needs to account for.\n",
    "\n",
    "Make sure the pipeline same as Carlos' feature extraction pipeline. Carlos applied a notch filter."
   ]
  },
  {
   "cell_type": "code",
   "id": "93f2d8accd887218",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-01-13T03:49:26.265083640Z",
     "start_time": "2025-01-07T07:46:40.361435Z"
    }
   },
   "source": [
    "# Plotting filters for cue -> emotion\n",
    "freq_filter, time_filter = compute_filter(frolich_data, normed_emotion_barycenter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf668a2b8b18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_filter(freq_filter)\n",
    "plot_time_filter(time_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d6b7787920a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_subj_matches = subj_subj_matching(emotion_data_psds_raw, frolich_data_psds_raw)\n",
    "freq_filter_subj_subj, time_filter_subj_subj = compute_filter_subj_subj(frolich_data, subj_subj_matches, psds=frolich_data_psds_raw)\n",
    "\n",
    "# print the matches nicely formatted\n",
    "print(subj_subj_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9360deffaaea0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = transform_data(frolich_data, time_filter)\n",
    "transformed_data_subj_subj = transform_data(frolich_data, time_filter_subj_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f07624a1abe9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the PSD of the transformed data (averaged over channels)\n",
    "plot_psd(transformed_data, title='Transformed Data Original CMMN Barycenter Formula')\n",
    "plot_psd(transformed_data_subj_subj, title='Transformed Data Subject to Subject Matching')\n",
    "plot_psd(frolich_data, psds=frolich_data_psds_raw, title='Frolich Data')\n",
    "plot_psd(emotion_data, psds=emotion_data_psds_raw, title='Emotion Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8967bed0f36444",
   "metadata": {},
   "source": [
    "TODO from last full meeting with Dr. B & Carlos:\n",
    "- [x] Eliminate implicit broadcasting, loop over every subject explicitly due to bug prone mismatches\n",
    "- [x] Store, load, average PSDs properly\n",
    "- [x] Reference the original paper's barycenter rule, use that instead for now.\n",
    "- [x] Subj - subj matching and filter calculation\n",
    "\n",
    "Questions for Dr. B and Carlos\n",
    "- What is most common? Averaging PSDs before or after taking the PSD? I noticed that they lead to some different results ... see cell above.\n",
    "- Filters look more reasonable to me, but do not understand why they are jagged and have a spike at the end of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef280e0784ff3544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between averaging channels and then taking PSD vs. taking PSD and then averaging channels, respectively\n",
    "\n",
    "# average before\n",
    "plot_psd(frolich_data, psds=None)\n",
    "\n",
    "# average after\n",
    "plot_psd(frolich_data, psds=frolich_data_psds_raw)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b737a061ec22ae67"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
